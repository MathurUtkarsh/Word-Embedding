# Word Embedding

This repo contains a notebook having word embedding using embedding layers in Keras.

1.) One Hot Representation

One-hot encoding is used in machine learning as a method to quantify categorical data. In short, this method produces a vector with length equal to the number of categories in the data set.  If a data point belongs to the ith category then components of this vector are assigned the value 0 except for the ith component, which is assigned a value of 1.  In this way one can keep track of the categories in a numerically meaningful way.
 

