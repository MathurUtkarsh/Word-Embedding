# Word Embedding

This repo contains a notebook having word embedding using embedding layers in Keras.

1.) One Hot Representation

One-hot encoding is used in machine learning as a method to quantify categorical data. In short, this method produces a vector with length equal to the number of categories in the data set.  If a data point belongs to the ith category then components of this vector are assigned the value 0 except for the ith component, which is assigned a value of 1.  In this way one can keep track of the categories in a numerically meaningful way.
 
A short glimpse of Representation.

![Screenshot 2022-11-16 011042](https://user-images.githubusercontent.com/78642104/202010665-d6f153f6-57ed-46ce-b28f-0ad25c299be9.png)
